# Basics
## 2023-02-04 Sat
### 카프카의 시작
* 카프카는 LinkedIn이 데이터 수집과 분배 아키텍쳐가 파편화되어 운영 상에 어려움을 겪으면서 시작되었다.
* 크게 데이터를 생성하는 소스 애플리케이션과 이를 소비하는 타겟 애플리케이션으로 분류했을 때, 아키텍쳐가 복잡해질수록 데이터 전송 라인의 관리는 어려워진다.
  * 이 경우, **타겟 애플리케이션 중 하나에 장애가 발생하면 이와 관련이 있는 모든 소스 애플리케이션이 함께 오동작**하게 된다.
* LinkedIn은 이러한 문제를 해결하기 위해 많은 상용 프레임워크와 오픈 소스를 사용해보았지만, 결과적으로 데이터 파이프라인의 복잡성을 낮추지는 못했다.
  * 이 과정에서 다양한 메시징 플랫폼과 ETL 도구를 적용했지만, 결과는 항상 같았다.
* 결국 LinkedIn에서는 이러한 복잡성을 해결하기 위한 새로운 시스템을 개발하였고, 카프카는 각 애플리케이션 간의 연결을 중앙집중적으로 관리하며 시작되었다.

### 카프카와 FIFO
* 카프카의 내부에서 논리적으로 관리되는 파티션의 동작은 FIFO 방식으로 동작하는 큐와 유사하다.
  * 정확히는 **카프카가 내부적으로 토픽을 관리하며, 각각의 토픽은 다시 하나 이상의 파티션으로 구성**될 수 있다.
  * 때문에 **소스 애플리케이션은 타겟 애플리케이션에 신경쓰기 보다는 일단 데이터를 카프카에 적재**하는 방식으로 동작하게 된다.
* **파티션에 데이터를 삽입하는 소스 애플리케이션은 프로듀서이며 큐로부터 데이터를 소비하는 타겟 애플리케이션은 컨슈머로 지칭**될 수 있다.
* 이렇듯 **프로듀서에 의해 카프카 토픽에 삽입되는 데이터는 토픽을 구성하는 파티션 중 하나에 삽입**된다.
  * 반면, 컨슈머는 자신이 구독한 여러 파티션으로부터 데이터를 가져가 적절한 방식으로 소비한다.
  * 이 때, **중요한 것은 컨슈머가 파티션으로부터 데이터를 조회하더라도 조회된 파티션의 데이터는 삭제되지 않는 다는 특징**이다.
  * 때문에 **FIFO 구조로 인해 데이터를 조회했더라도, 해당 데이터는 다른 방식으로 다시 소비될 가능성**이 열려있다.
* **임의의 컨슈머가 데이터를 읽어들인 기록은 카프카 내부적으로 관리되며, 이러한 기록을 커밋이라는 용어로 지칭**한다.
  * 때문에 커밋을 토대로 컨슈머는 파티션의 어느 지점까지 조회했고, 어디부터 조회해야할지 알 수 있다.

## 2023-02-05 Sun
### 카프카의 특징
* 카프카는 여러 용도 중에서도 주로 데이터 파이프라인 용도로 사용되며, 이유는 크게 다음과 같은 네가지로 분류할 수 있다.
  1. 높은 처리량
  2. 높은 확장성
  3. 높은 영속성
  4. 높은 가용성
* 우선, **카프카는 프로듀서 또는 컨슈머가 데이터를 활용하는데에 있어 묶음으로 전송하는 특징이 있어 낮은 네트워크 IO와 높은 처리량이 보장**된다.
  * 이 경우, 많은 양의 데이터를 묶어 처리하므로 대용량의 실시간성 데이터를 처리하는 데에 적합하다.
  * 나아가 동일한 목적의 데이터를 여러 파티션에 분배하여 데이터를 병렬로 처리할 수 있으며, 파티션 수만큼 컨슈머 수를 늘려 처리량을 더욱 높일 수 있다.
  * 이는 일반적으로 하나의 파티션에 적재되는 데이터를 하나의 컨슈머가 소비하기 때문이며, 컨슈머의 사양을 높이는 것보다 그 수를 늘리는 것이 효과적일 수 있다.
  * 결국, **카프카의 높은 처리량을 보장하는 근간에는 아키텍쳐에 따라 컨슈머를 손쉽게 스케일 아웃할 수 있다는 특징**에 있다.
* **데이터 파이프라인에서 데이터를 수집할 경우 데이터의 정확한 양을 예측하기는 쉽지 않은 반면, 카프카는 가변적인 환경에서도 안정적으로 확장이 가능**하다.
  * 예를 들어, 데이터가 적다면 카프카 클러스터의 브로커는 최소한의 개수로 운영되지만 데이터가 늘어난다면 브로커의 개수를 손쉽게 스케일 아웃할 수 있다.
  * 반면 다시 **데이터의 수가 적어진다면 브로커의 개수가 자연스레 스케일 인되며, 나아가 이러한 모든 과정은 무중단 운영을 지원**한다.
* **카프카는 다른 메시징 플랫폼과 달리 메시지를 메모리가 아닌 파일 시스템에 저장하는 한 편, OS 차원에서 파일 시스템을 최대한 활용하여 성능을 보장**한다. 
  * 예를 들어 **OS에서는 디스크 IO 성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성하여 사용**한다.
  * 이렇듯 페이지 캐시를 활용하는 카프카는 한 번 조회한 데이터는 메모리에 적재하여 재사용하며, 브로커가 예기치 못하게 종료되더라도 데이터는 유실되지 않는다.
* **카프카 클러스터는 기본적으로 셋 이상의 서버로 구성되어 운영되므로, 일부 서버에 장애가 발생하더라도 무중단으로 운영을 지속**할 수 있다.
  * 이렇듯 **클러스터로 구성되는 카프카는 하나의 브로커로 전송된 데이터를 또 다른 브로커에도 저장하는 데이터 복제를 활용하여 높은 고가용성을 보장**한다.
  * 또한 특정한 브로커 옵션을 통해 온프레미스 환경의 서버 랙함 또는 클라우드 상의 리전 단위의 장애 상황에서도 데이터를 안전하게 복제할 수 있다.

## 2023-02-06 Mon
### 카프카의 생태계
* 카프카 생태계는 크게 다음과 같은 세 구성 요소로 분류할 수 있다.
  1. 프로듀서: 카프카 클러스터에 데이터를 삽입한다.
  2. 클러스터: 여러 주제 별 토픽으로 나뉘어 데이터를 적재한다.
  3. 컨슈머: 카프카 클러스터의 토픽으로부터 데이터를 가져가 소비한다.
* 또한, 상술한 핵심 구성 요소 외에도 다음과 같은 구성 요소들이 포함될 수 있다.
  1. 스트림즈: 임의의 토픽으로부터 데이터를 전처리하여 새로운 토픽에 삽입하고자하는 경우에 사용한다.
  2. 커넥트: 데이터 파이프라인을 구성하는 가장 핵심적인 툴이며, 소스 커넥터와 싱크 커넥터로 분류된다.
     1. 소스 커넥트: **프로듀서 역할을 수행하며, 임의의 데이터베이스나 오브젝트 스토리지로부터 데이터를 가져와 임의의 토픽에 적재**하게 된다.
     2. 싱크 커넥트: **컨슈머 역할을 수행하며, 임의의 토픽으로부터 JDBC나 ES와 같은 타겟 애플리케이션으로 데이터를 전송**하게 된다.
* 이 때, **상술한 모든 개념은 오픈 소스 프로젝트인 카프카에 포함되는 일종의 Java 라이브러리**로 이해할 수 있다.
  * 즉, 아파치 카프카 공식 문서에 제공되는 모든 기능이나 상술한 구성 요소를 조합하는 아키텍쳐의 구현은 동일한 오픈 소스 프로젝트를 기반으로 정의된다.
  * 반면, 서드 파티 라이브러리를 아키텍쳐에 포함시키게 되는 시점부터는 카프카의 모든 요소를 최대한 활용할 수 있음을 보장받지 못하게 되기 쉽다.

### 카프카 브로커란?
* **카프카 브로커는 프로듀서로부터 데이터를 전송하고, 컨슈머가 이를 소비하기 위해서 반드시 거쳐가게 되는 프로세스**이다.
  * 즉, 브로커는 카프카 클라이언트와 데이터를 주고받기 위해 사용하는 요소이자 데이터를 분산 저장하여 장애에도 안전하게 운영될 수 있도록 하는 애플리케이션이다.
* **하나의 서버에는 하나의 카프카 브로커 프로세스가 실행**되며, 최소한 하나의 브로커 서버만으로도 카프카의 기본 기능을 사용할 수 있다.
  * 그러나 **데이터를 안전하게 보관하고 처리하기 위해서는 최소한 셋 이상의 브로커 서버를 묶어 하나의 클러스터로 취급하여 운영하는 것이 바람직**하다.
  * 이렇듯 **클러스터로 묶인 카프카 브로커 서버들은 프로듀서가 전송한 데이터를 안전하게 복제하고 분산 저장하는 역할을 수행**한다.

### 카프카 클러스터란?
* **카프카 클러스터는 둘 이상의 카프카 브로커 서버로 구성되며, 각각의 브로커 서버는 물리적인 단일 서버 또는 하나의 인스턴스로부터 실행**된다.
  * 운영 환경에서는 최소한 셋 이상의 브로커 서버로 클러스터를 구성하는 것이 일반적이다. 
  * 반면, 카프카 클러스터에서 사용되는 데이터의 양이 많아지는 경우에는 50에서 100개의 브로커 서버를 사용하거나 용도에 따라 클러스터를 여럿 운영할 수 있다.

### 주키퍼란?
* 주키퍼는 카프카 클러스터를 운영하기 위해 필수적인 구성 요소이다.
  * 카프카 2.x 버전까지는 주키퍼가 필수였으며, 3.x 버전부터는 주키퍼가 없어도 운영되는 기능들이 있으나 아직까지는 필수적인 구성 요소로 보는 것이 권장된다.
  * 때문에 많은 IT 기업의 운영 환경에서는 여전히 카프카 클러스터의 운영을 위해 주키퍼가 필수적으로 포함되는 경우가 많다.

## 2023-02-07 Tue
### 카프카 클러스터와 주키퍼
* 기본적으로 카프카 클러스터의 경우에는 여럿을 동시에 운영하는 경우도 잦다.
  * 예를 들어 **세 개의 카프카 클러스터를 운영하는 경우, 각각의 클러스터가 모두 여러 주키퍼로 구성되는 하나의 주키퍼 앙상블에 등록**된다.
* 상술한 바와 같이 카프카 3.x 버전 이전까지는 클러스터를 실행하기 위해서는 주키퍼가 필수적이며, 주키퍼의 서로 다른 znode에 클러스터를 지정하게 된다.
  * 반면, 3.x 버전부터는 브로커들을 따로 운영하여 하나의 클러스터를 구성할 때 주키퍼를 사용하지 않을 수 있으나 아직까지 상용 성공 사례는 없다.
* 이는 결국 하나의 주키퍼 앙상블로 여러 클러스터를 관리하는 것이므로, 다음과 같은 tradeoff가 있을 수 있다.
  1. 해당 방식을 사용할 경우, 하나의 주키퍼 앙상블의 상태에 모든 클러스터의 운영이 종속된다.
  2. 해당 방식을 사용하지 않는 경우, 주키퍼 앙상블을 클러스터마다 운영하기 위해 불필요한 리소스가 낭비될 수 있다.

## 2023-02-08 Wed
### 카프카 브로커의 역할
* 클러스터를 구성하는 브로커들은 크게 다음과 같은 역할을 수행한다.
  1. 컨트롤러: 클러스터에 포함되는 브로커 중 하나가 역할을 수행하며, 컨트롤러가 역할을 수행하지 못하는 경우 다른 브로커가 컨트롤러로 승격된다.
  2. 데이터 삭제: **데이터 삭제는 오직 브로커에 의해서만 처리될 수 있으며, 이는 로그 세그먼트라고 불리우는 파일 단위로 수행**된다. 
  3. 컨슈머 오프셋 저장: **컨슈머 그룹은 브로커의 `__consumer_offsets` 토픽에 저장된 오프셋을 토대로 다음에 소비할 데이터를 확인**할 수 있다.  
  4. 그룹 코디네이터: 코디네이터는 컨슈머 그룹의 상태를 체크하고, 각 파티션을 임의의 컨슈머에 분배하여 매칭하는 역할을 수행한다.
* 특히 **컨트롤러는 다른 브로커들의 상태를 체크하며, 브로커 중 하나가 클러스터로부터 제거되는 경우 해당 브로커의 리더 파티션을 재분배**한다.
  * 카프카는 지속적으로 데이터를 처리하므로, 이렇듯 비정상적인 브로커를 지속적으로 체크하고 클러스터로부터 제거하는 것은 매우 중요하다.
* 카프카는 다른 플랫폼과 달리 컨슈머가 데이터를 소비하더라도 이를 삭제하지 않으며, 컨슈머 또는 프로듀서가 명시적인 데이터 삭제를 요청할 수도 없다.
  * 이 때, **삭제되는 로그 세그먼트에는 다수의 데이터가 포함되므로 RDBMS와 같은 특정 데이터 선별 및 삭제는 불가능**하다.
* 또한, **컨슈머 그룹은 토픽의 특정 파티션으로부터 데이터를 가져가 소비한 후 해당 파티션의 어느 데이터까지 소비했는지를 오프셋으로 커밋**한다.
  * 컨슈머 그룹을 생성하고 운영하는 이러한 과정에서 최초로 커밋을 수행한 경우, `__consumer_offsets` 토픽은 인터널 토픽으로서 자동으로 생성된다.
  * 때문에 **이러한 인터널 토픽의 경우 사용자가 이를 인지할 필요가 없으며, 해당 토픽에 데이터를 쓸 필요도 없는 것이 일반적**이다.
* **코디네이터는 컨슈머가 컨슈머 그룹에서 제거된 경우, 매칭되지 않은 파티션을 정상 동작하는 컨슈머에게 재할당하여 끊임없이 데이터가 처리될 수 있도록 한다**.
  * 이 때, 이렇듯 **파티션을 정상적인 컨슈머에 재분배하는 작업을 리밸런스라고 지칭하며 각 파티션은 기본적으로 컨슈머와 1:1 관계로 매칭**된다.
  * 그러나 임의의 컨슈머에 이슈가 발생하거나 명시적으로 컨슈머 그룹으로부터 제거된 경우, 리밸런스 과정에서 컨슈머는 1:N 관계로 파티션과 매칭될 수도 있다.

## 2023-02-09 Thu
### 브로커의 데이터 저장 위치
* **카프카 브로커의 핵심 역할은 데이터를 저장하는 것으로, 로그는 `config/server.properties`의 log.dir 옵션에 정의한 디렉토리에 저장**된다.
  * 이렇듯 **데이터는 파일 시스템으로 관리되며, 토픽의 이름과 파티션 번호를 조합한 하위 디렉토리를 생성한 후 데이터를 저장**한다.
  * 예를 들어 hello 토픽의 0번 파티션에 저장되는 로그는 `hello-0` 디렉토리에 저장된다.
* 또한, 상술한 디렉토리에 저장되는 파일은 크게 다음과 같이 분류할 수 있다.
  1. `.index`: 메시지 오프셋을 인덱싱한 정보가 저장된다.
  2. `.log`: 메시지와 메타데이터가 저장된다.
  3. `.timeindex`: 메시지에 포함되는 timestamp를 기준으로 인덱싱된 정보가 저장된다.
* 이 때, **메시지는 카프카 프로듀서가 전송한 하나의 레코드를 의미하며 정식적인 용어 역시 레코드**이다.

### 카프카 로그와 세그먼트
* **카프카의 로그는 각각 여러 세그먼트로 구성**되며, 이와 관련된 설정 중 기억할 만한 것은 다음과 같다.
  1. `log.segment.bytes`: 바이트 단위로 최대 세그먼트 크기를 지정하며, 기본적으로는 1GB로 설정된다.
  2. `log.roll.ms(또는 hours)`: 세그먼트가 신규 생성된 이후부터 따져 다음 파일로 전환되는 시간 주기이며, 기본 값은 7일로 설정된다.
* 이렇듯 **카프카의 로그는 세그먼트라는 여러 파일로 분할된 채 관리되며, 이 때 각 세그먼트를 나누는 기준이 되는 것은 상술한 두 가지 설정**이 된다.
  * 이러한 과정에 의해 **분리된 세그먼트에는 더 이상 프로듀서의 의 데이터가 저장되지 않고, 하술할 액티브 세그먼트에만 데이터가 지속적으로 저장**된다.
* **카프카 로그를 구성하는 각 세그먼트의 데이터 단위는 오프셋이라는 용어로 지칭되며, 해당 세그먼트의 시작 오프셋 주소가 세그먼트 파일의 이름에 사용**된다.
  * 예를 들어, `000010.log` 파일은 오프셋 10부터 시작하는 세그먼트 파일로 이해할 수 있으므로 세그먼트 파일의 이름만으로 많은 정보를 얻을 수 있다.
  * 이 때, 오프셋은 각 레코드를 식별하는 고유한 번호를 의미하며 마치 큐와 같은 원리로 고유한 오프셋을 가진 레코드들이 세그먼트에 지속적으로 저장된다.
* 각 세그먼트 파일은 세그먼트 생성 시점으로부터 일정 시간이 지났을 때 분리되기도 하므로, 데이터의 삽입 빈도에 따라 세그먼트 파일의 크기는 제각각일 수 있다.

### 액티브 세그먼트란?
* **카프카 로그를 구성하는 세그먼트 중 가장 마지막 세그먼트 파일로, 쓰기 대상이 되는 최신 세그먼트 파일을 액티브 세그먼트라는 용어로 지칭**한다.
  * 반면, 나머지 세그먼트들은 일반 세그먼트라고 지칭한다.
* 때문에 액티브 세그먼트는 브로커의 삭제 대상이 되지 않는 반면, 일반 세그먼트들은 `retention` 옵션에 따라 삭제 대상이 될 수 있다.
  * 이는 데이터가 소비되더라도 즉시 제거하지는 않는 카프카의 특성에서 기인하며, 데이터를 무한정 저장할 수 없으므로 지속적으로 세그먼트를 정리할 필요가 있다.

## 2023-02-10 Fri
### 세그먼트의 삭제 주기
* 세그먼트의 삭제 주기와 관련된 설정 역시 존재하며, 그 중 `cleanup.policy=delete` 정책에서 중요한 설정들은 크게 다음과 같이 분류할 수 있다.
  1. `retention.ms(또는 minutes, hours)`: 세그먼트를 보유하는 최대 기간을 의미하며, 기본적으로는 7일로 설정된다.
  2. `retention.bytes`: **파티션 당 로그가 적재되는 바이트 값을 의미**하며, 기본적으로는 -1이 적용되어 지정하지 않은 것과 같다.
  3. `log.retention.check.interval.ms`: **세그먼트가 삭제 영역에 진입했는지 확인하는 간격이며, 기본적으로는 5분이 적용**된다.
* 이 때, **`cleanup.policy=delete` 정책은 액티브가 아닌 일반 세그먼트 파일 중 조건을 충족하는 세그먼트를 제거하는 방식으로 동작**한다.
  * 반면, `retention.bytes` **조건이 충족되면 일반 세그먼트를 모두 제거하지만 액티브 세그먼는 삭제 대상이 아니므로 정상적인 운영이 가능**하다.
* 또한 서비스 유형에 따라 더 낮은 `retention.ms` 설정 값을 적용하는 경우가 있을 수 있으며, 일반적으로는 주말에도 정상 운영될 수 있도록 3일을 적용한다.
  * 예를 들어, **토픽에 많은 데이터가 적재될 것으로 예상되는 서비스의 경우에는 해당 설정 값을 낮게 적용하는 것이 바람직**하다.
  * 나아가 토픽에 적재되는 데이터의 양과 카프카 브로커의 파일 시스템 용량으로 미루어 적절한 설정 값을 결정하고, 이를 브로커 서버에 적용할 수 있어야 한다.

### 데이터 적재 및 소비 시 유효성 검증의 필요성
```
> 세그먼트는 각 오프셋을 의미하는 레코드들로 구성되며, 세그먼트는 다시 파티션의 데이터를 구성한다. 
```
* **카프카의 경우, 데이터는 항상 세그먼트 단위로 삭제되므로 로그 단위(또는 레코드 단위)의 개별 삭제가 불가능**하다.
  * 예를 들어 특정한 토픽의 파티션에서, 세그먼트에 포함된 개 별 레코드 중 오프셋 1번만을 제거할 수는 없다.
  * 이에 더해 **로그(또는 레코드)의 메시지 키와 값, 오프셋과 헤더 등 이미 저장된 데이터는 수정 역시 불가능**하다.
* 상술한 이유에서, **프로듀서가 데이터를 적재하는 시점 또는 컨슈머가 데이터를 사용하는 시점에는 반드시 데이터의 유효성을 검증하는 것이 바람직**하다.
  * 이 때, **카프카 클러스터 또는 전체적인 카프카의 구조상 레코드가 적재되는 시점에 유효성을 검증하는 기능은 아직까지 지원되지 않는다**.

## 2023-02-11 Sat
### 토픽 압축 정책
* 세그먼트 관련 정책 중, `cleanup.policy=compact`의 경우 기본적으로 압축 정책을 의미한다.
  * 이 때, **해당 정책은 compressing과는 다른 개념이며 메시지 키 별로 해당 메시지 키가 갖는 레코드 중 오래된 데이터만을 삭제**한다.
* 이렇듯 **해당 정책은 delete 정책과는 다르게 일부의 레코드만이 삭제될 수 있는 반면, 마찬가지로 일반 세그먼트의 데이터만이 삭제 대상**이 된다.
  * 때문에 액티브 세그먼트와 일반 세그먼트에 동일한 메시지 키가 존재하더라도, 다른 세그먼트이므로 같은 메시지 키를 갖는 레코드가 여럿 존재할 수 있다.
* 예를 들어 같은 키를 갖는 메시지들이더라도 오프셋은 다를 수 있으며, 해당 정책의 경우 낮은 값의 오프셋을 갖는 레코드가 우선적으로 제거된다.
* 해당 정책을 활용할 경우 각 메시지 키 별로 최신의 레코드만을 남기므로, 마치 키-값 저장소처럼 취급하여 배치 데이터로 처리하는 것이 용이할 수 있다.

### 테일 영역과 헤드 영역, 클린 로그와 더티 로그
```
> 압축 정책을 적용한 경우, 압축 대상이 되지 않는 액티브 세그먼트를 제외한 일반 세그먼트들은 다시 테일 영역과 헤드 영역으로 분류할 수 있다.  
```
* **세그먼트들을 구성하는 여러 레코드 중, 상술한 압축 정책에 의해 압축이 완료된 레코드들을 테일 영역 또는 클린 로그로 지칭**한다.
  * 즉, 해당 영역은 압축 대상이 되었으므로 중복 메시지 키가 존재할 수 없는 반면 압축 과정에서 제거되는 레코드로 인해 오프셋은 연속적이지 않을 수 있다.
* 반면, **압축 정책이 아직 적용되기 전의 레코드들은 헤드 영역 또는 더티 로그라고 지칭하며 중복된 메시지 키가 존재**할 수 있다.
  * 이 경우, 테일 영역과는 반대로 헤드 영역에 포함된 레코드들의 오프셋은 연속적인 값을 갖게 된다.

### 압축 시점의 결정
* 압축 정책에서, 데이터의 압축 시점은 `min.cleanable.dirty.ratio` 설정 값을 따라 결정된다.
  * 이 때, **해당 설정 값은 액티브 세그먼트를 제외한 일반 세그먼트에 남은 테일 영역의 레코드 수와 헤드 영역의 레코드 수의 비율을 의미**한다.
  * 예를 들어, 0.5로 설정된 경우 테일 영역의 레코드 개수와 헤드 영역의 레코드 개수가 같아지는 시점에 압축이 진행된다.
* 해당 설정을 **0.9와 같이 큰 값으로 설정할 경우, 한 번 압축할 때 많은 데이터가 제거될 것이므로 압축 효율은 높아진다**.
  * 그러나 압축 시점까지 더 많은 데이터를 저장하게 되므로 용량적인 측면과 오래된 데이터를 유지하게 된다는 tradeoff가 존재한다.
* 반면 **0.1과 같이 작은 값으로 설정할 경우, 압축이 자주 실행되어 최신 데이터만을 유지할 수 있는 반면 압축 과정으로 인해 브로커에 부담**을 줄 수 있다.

### 카프카 브로커와 복제
* **카프카 브로커가 제공하는 데이터 복제는 카프카가 장애 허용 시스템으로 동작할 수 있도록 하는 기반**과도 같다.
  * 때문에 복제는 카프카 클러스터를 운영하는 데에 있어 매우 중요하며, 클러스터의 브로커 중 일부에 장애가 발생하더라도 데이터의 유실 없이 사용이 가능하다.
* **카프카의 데이터 복제는 파티션 단위로 진행되며, 토픽 생성 시 파티션 복제 개수를 함께 설정할 수 있다**.
  * 반면, 옵션을 선택하지 않은 경우에는 브로커에 설정된 옵션 값을 따른다.
* **복제 개수, 즉 replication factor 설정의 최소값은 복제를 하지 않는 1인 반면 최대값은 브로커의 개수**와 같다.
* 그러나, 운영 환경의 경우 복제는 클러스터에 포함된 파티션의 개수를 그대로 따르기보다는 둘 또는 세 개의 클러스터만을 사용하다.
  * 또한, 데이터의 종류에 따라 서로 다른 복제 개수를 설정하거나 토픽 별 복제 개수를 달리 설정할 수도 있다.
  * 예를 들어, 메트릭과 같이 데이터가 유실되어도 무방하지만 데이터의 처리 속도가 중요한 경우에는 복제 수는 1이 적절한 반면 일반적인 경우에는 2를 설정한다.
  * 반면, 금융 정보와 같이 절대 유실이 발생하지 않아야하는 데이터의 경우에는 복제 수를 3으로 설정하는 것이 권장된다.
* 토픽 별로 다른 복제 수를 유지하기 위해 토픽을 생성하는 시점에서 복제 수를 설정할 수도 있으며, 설정하지 않는 경우에는 브로커 서버의 기본 설정을 따른다.

### 리더 파티션과 팔로워 파티션
* 상술한 카프카 브로커의 복제 기능에서, 복제된 파티션은 리더 파티션과 팔로워 파티션으로 구분된다.
  * 이 때, **프로듀서 또는 컨슈머와 직접 통신하는 파티션은 리더 파티션이며 나머지 복제 데이터를 갖는 파티션은 팔로워**로 지칭한다.
  * 때문에 프로듀서가 적재한 데이터는 기본적으로 리더 파티션에 우선 저장되며, 이후 팔로워 파티션들이 해당 정보를 자신에게 반영하게 된다.
* 팔로워 파티션의 경우, 리더 파티션의 오프셋을 확인하여 현재 자신이 갖는 오프셋과 차이가 있을 때 리더 파티션의 레코드를 자신의 파티션에 복제한다.
  * 반면, 이러한 과정에서 모든 브로커에 파티션의 데이터가 복제되므로 복제본의 수만큼 저장 용량이 증가한다는 단점 역시 존재한다.
  * 그러나 기업용 서버의 경우 PC에 비해 안정성이 높지만, 여러 내외부 요인에 의해 언제든지 장애가 발생할 위험 역시 존재한다.
  * 따라서 **복제를 통해 얻을 수 있는 안정성의 가치가 훨씬 크기에 카프카 클러스터를 구성하는 경우 둘 이상의 복제를 유지하는 것이 바람직**하다.

## 2023-02-12 Sun
### 브로커 장애 발생에 따른 팔로워 파티션의 승격 과정
* 임의의 리더 파티션을 갖는 브로커 서버에 장애가 발생한 경우, 해당 브로커의 리더 파티션은 사용이 불가능하므로 팔로워 중 하나가 리더로 승격된다.
  * 이러한 동작 방식으로 인해 데이터는 유실되지 않으며, 컨슈머와 프로듀서와의 상호작용 역시 지속적으로 이어질 수 있다.

### In Sync Replicas란?
```
> ISR은 리더 파티션과 팔로워 파티션의 동기화가 완료되어 같은 수의 오프셋을 갖는 상태를 의미한다.
```
* 예를 들어 복제 수가 2인 토픽의 경우, 리더 파티션과 팔로워 파티션은 각 1개 씩 존재하게 된다.
  * 이 때 리더 파티션에 0, 1, 2, 3 오프셋이 존재한다면 팔로워 파티션에도 해당 오프셋이 모두 존재해야 동기화가 완료된 것이 된다.
  * 그러나 리더 파티션의 정보가 팔로워에게 미쳐 복제되지 않은 시점에 장애가 발생하여 팔로워 파티션이 리더로 승격된 경우, 데이터가 유실될 가능성이 있다.
* 이 경우, 리더 파티션이 선출되는 과정에서 ISR 여부에 따라 데이터의 유실을 감수할지 결정한 후에 다음과 같은 설정을 적용할 수 있다.
  1. `unclean.leader.election.enable=true`: **유실을 감수하며, 미처 복제가 되지 않은 팔로워 파티션이라도 리더로 승격**시킨다.
  2. `unclean.leader.election.enable=false`: **유실을 감수하지 않으며, 리더 파티션이 포함된 브로커가 복구될 때까지 서비스를 중단**한다.
* 이렇듯 **상술한 설정을 통해 장애가 발생한 경우 유실을 감수하되 서비스를 지속적으로 운영할지, 서비스의 복구를 기다릴지 선택**할 수 있다.
  * 예를 들어, 반드시 24/7이 보장되어야 하는 서비스라면 해당 옵션을 true로 설정해야할 수 있다.

### 토픽과 파티션
```
> 토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위이다.
> 토픽은 하나 이상의 파티션을 소유하며, 파티션에는 프로듀서가 전송한 데이터들이 저장된다.
```
* **파티션에 저장된 데이터들은 레코드라는 용어로 지칭되며, 파티션은 마치 큐와 유사한 구조**를 갖는다.
  * 즉, FIFO 구조를 따르므로 먼저 삽입된 레코드가 먼저 소비된다.
  * 그러나 **일반적인 큐와 달리 데이터를 소비하더라도 데이터가 실제로는 삭제되지는 않는다.
* **파티션의 레코드는 컨슈머의 소비와는 별개로 관리되며, 이러한 특징으로 인해 토픽의 레코드는 여러 목적을 갖는 컨슈머 그룹들에 의해 여러번 처리**될 수 있다. 

### 토픽의 생성과 파티션의 배치
* 다수의 파티션을 갖는 토픽을 생성한 경우, 0번 브로커 서버를 기준으로 round-robin 방식에 의해 파티션들이 배치된다.
* 카프카 클라이언트는 리더 파티션이 있는 브로커와 통신하여 데이터를 주고 받으므로, 여러 브로커와 고르게 통신하게 된다.
  * 예를 들어 **파티션이 다섯 개인 토픽을 생성한 경우, 이들 파티션은 모두 리더 파티션인 반면 여러 브로커 서버에 고르게 배치**된다. 
  * 때문에 **데이터가 특정 서버에만 집중되는 핫 스팟 현상에서 자유로울 수 있으며, 데이터의 수가 많아지더라도 자연스러운 대응이 가능**하다.
* 반면, 복제 기능을 활용하는 경우 round-robin 방식으로 분배된 모든 파티션 각각에 대한 복제본이 생성된다.
  * 예를 들어 **파티션이 다섯 개인 토픽의 경우, 복제본을 포함한 파티션의 수는 파티션 수 * 브로커 서버의 수**가 된다.